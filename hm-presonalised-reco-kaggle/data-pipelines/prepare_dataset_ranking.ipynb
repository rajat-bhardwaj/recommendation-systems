{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05da68fa-3d9a-4ae7-9a1c-1a1bae12764d",
   "metadata": {},
   "source": [
    "## Prepare dataset \n",
    "\n",
    "- Candidate Selection (Retrieval): Matrix Factorisation; Generate `n` negative per positive example. (Weekly)\n",
    "- Ranking: (Hybrid); New derived features for both customer and products.\n",
    "- Development set: Training and Validation; train from `2018 week-38` to `2020 Week-32`\n",
    "- Test set: From `2020 Week-33` to `2020 Week-39`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164cac29-03da-46d8-a74c-2219cc29c8de",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2be14-c9ea-46c8-bd03-e88d9236d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d84bee9-6c6d-4e6d-a74d-92dae1444458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819c280-cf91-4f3c-8864-98133862fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import _read_data_files_helper as data_files\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# setup pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=6)\n",
    "\n",
    "tqdm.pandas(desc=\"Negative samples!\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c29f29-4e42-4c5e-ae3c-964b55e1ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path(\"\") ## original csv file path\n",
    "path_model_data = Path(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7dc4d0-aa9a-4b8c-9d89-2153903f2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# %%memit\n",
    "\n",
    "# df_customers = data_files.read_customer_file(path_base)\n",
    "# df_transactions = data_files.read_transactions(path_base)\n",
    "# df_products = data_files.read_articles(path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0610a-65d5-4cb5-a22a-ac370b195cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df_products.apply(lambda x: x.isna().sum()).sum() == 416,  #known missing product description\n",
    "# df_customers.apply(lambda x: x.isna().sum()).sum() == 0, df_transactions.apply(lambda x: x.isna().sum()).sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc76c7a-1059-448d-888a-7f78bc2f99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_customers.shape, df_transactions.shape, df_products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577e4c1-51df-4e1a-8891-6ab00e057244",
   "metadata": {},
   "source": [
    "### Ranking: \n",
    "\n",
    "Generate customer and product features from transaction table\n",
    "\n",
    "Development set\n",
    "- train set: 2018 week-38 to 2020 Week-28 (max date: 2020-07-12)\n",
    "- validation set: 2020 Week-29 to 2020 Week-32 (4 weeks of validation data)\n",
    "- i.e. train model on historic data of two years and evaluate performance on upcoming week(s)\n",
    "\n",
    "Features\n",
    "- for each customer and product with their last active date in the train set,\n",
    "- Compute features over 3 months, 6-months, one year and lifetime period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc54fb3-03d8-4394-9c18-6ff2f5df967f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute max date in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b44664-46f5-4352-b984-002249f2e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_active_date(input_df, feature, df_name) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the last active date of given feature\n",
    "    and merge it with original dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df : Pandas DataFrame\n",
    "    feature : String\n",
    "        Feature name to run groupby\n",
    "    df_name: String\n",
    "        Name of the dataset (Train, Validation or Test)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "    \"\"\"\n",
    "    col_name = df_name + \"_\" + feature + \"_max_dt\"\n",
    "    results = (\n",
    "        input_df.groupby(feature, as_index=False)\n",
    "        .agg({\"t_dat\": \"max\"})\n",
    "        .rename(columns={\"t_dat\": col_name})\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6374efc7-4726-4986-9eac-344f8c6d605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_max_date(row, path_base, df_transactions):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df : Pandas DataFrame\n",
    "    feature : String\n",
    "        Feature name to run groupby\n",
    "    df_name: String\n",
    "        Name of the dataset (Train, Validation or Test)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_name = row.df_name.iloc[0]\n",
    "    feature = row.feature.iloc[0]\n",
    "\n",
    "    train_df, val_df, test_df = data_files.get_model_dfs(path_base, df_transactions)\n",
    "\n",
    "    if df_name == \"train\":\n",
    "        data_ = train_df\n",
    "    elif df_name == \"val\":\n",
    "        data_ = val_df\n",
    "    elif df_name == \"test\":\n",
    "        data_ = test_df\n",
    "\n",
    "    # print(\"#Customers: \" + df_name , data_.customer_id.nunique())\n",
    "    # print(\"#Articles:  \" + df_name, data_.article_id.nunique())\n",
    "\n",
    "    df_ = get_last_active_date(data_, feature, df_name)\n",
    "\n",
    "    assert df_[feature].nunique() == data_[feature].nunique()\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b5806-0e9c-4b63-9272-e20fdd5d5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_df(path_base, path_model_data):\n",
    "    \"\"\" \"\"\"\n",
    "    dummy_max_date_df = pd.DataFrame(\n",
    "        [\n",
    "            (df_name, feat)\n",
    "            for df_name in [\"train\", \"val\", \"test\"]\n",
    "            for feat in [\"customer_id\", \"article_id\"]\n",
    "        ],\n",
    "        columns=[\"df_name\", \"feature\"],\n",
    "    )\n",
    "\n",
    "    ## adding only age column as only age feature is used.\n",
    "    ## If there are more maual features one can join both customer and product features\n",
    "    df_customers = data_files.read_customer_file(path_base)\n",
    "    df_transactions = data_files.read_transactions(path_base)\n",
    "    df_combined = df_transactions.merge(\n",
    "        df_customers.filter([\"customer_id\", \"age\"]), on=\"customer_id\"\n",
    "    )\n",
    "\n",
    "    org_rows = df_combined.shape[0]\n",
    "    org_columns = df_combined.shape[1]\n",
    "\n",
    "    # run parallel run for max date computation\n",
    "\n",
    "    res_ = dummy_max_date_df.groupby([\"df_name\", \"feature\"]).parallel_apply(\n",
    "        lambda x: run_max_date(x, path_base, df_transactions.copy())\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(dummy_max_date_df.itertuples()):\n",
    "        df_name = row.df_name\n",
    "        feature = row.feature\n",
    "        df_ = (\n",
    "            res_.loc[(df_name, feature)]\n",
    "            .dropna(axis=1, how=\"any\")\n",
    "            .astype({feature: \"int64\"})\n",
    "        )\n",
    "\n",
    "        df_combined = df_combined.merge(df_, on=feature, how=\"left\")\n",
    "\n",
    "        ## there will be null values for customers not in other datasets\n",
    "        ## for example customers outside training dataset will have null for train_customer_max_dt)\n",
    "        # assert df_combined.isna().sum().sum() == 0\n",
    "\n",
    "        assert df_combined.shape[0] == org_rows\n",
    "        assert df_combined.shape[1] == org_columns + i + 1\n",
    "\n",
    "    ## there will be missing values in the newly created date columns\n",
    "    df_combined.to_parquet(path_model_data / \"combined_dataframe.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b50c6-5d9e-4023-a36b-2616da2fba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "_ = get_dates_df(path_base, path_model_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c17a1-c222-427a-871b-488cf6215bca",
   "metadata": {},
   "source": [
    "#### Generate features\n",
    "\n",
    "- Train, validate and test dataset are partitioned by time. \n",
    "- Features are split over time as follows lifetime, 12 months, 6 months, 3 months for both customer and product.\n",
    "- To run parallel execution, each process will\n",
    "    - read the original transaction file\n",
    "    - filter dataframe for each duration ( 4 partitions )\n",
    "    - compute fratures for each subset of data\n",
    "    - process the output to store as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c676ac-0db2-4dbc-8d07-597f46c6ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.DataFrame(\n",
    "    [\n",
    "        [\"customer_id\", \"train_customer_id_max_dt\"],\n",
    "        [\"customer_id\", \"val_customer_id_max_dt\"],\n",
    "        [\"customer_id\", \"test_customer_id_max_dt\"],\n",
    "        [\"article_id\", \"train_article_id_max_dt\"],\n",
    "        [\"article_id\", \"val_article_id_max_dt\"],\n",
    "        [\"article_id\", \"test_article_id_max_dt\"],\n",
    "    ],\n",
    "    columns=[\"agg_feature\", \"date_feature\"],\n",
    ").reset_index()\n",
    "\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72f03d-7bbd-48d4-bbee-313a96aa4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_data(path_model_data, sample=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads data from a parquet file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_model_data : str\n",
    "        filepath to store model input data\n",
    "    sample : bool, optional\n",
    "        A flag to generate a small sample for testing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_ = (\n",
    "        pd.read_parquet(\n",
    "            path_model_data / \"combined_dataframe.parquet\",\n",
    "        )\n",
    "        # nrows = 1000 if sample else None,\n",
    "        # parse_dates=['t_dat', 'train_customer_id_max_dt',\n",
    "        #              'val_customer_id_max_dt', 'test_customer_id_max_dt',\n",
    "        #              'train_article_id_max_dt', 'val_article_id_max_dt',\n",
    "        #              'test_article_id_max_dt'])\n",
    "        .astype(\n",
    "            {\n",
    "                \"customer_id\": \"int64\",\n",
    "                \"article_id\": \"int64\",\n",
    "                \"tx_year\": \"category\",\n",
    "                \"tx_month\": \"int8\",\n",
    "                \"tx_week\": \"int8\",\n",
    "                \"price\": \"float32\",\n",
    "                \"sales_channel_id\": \"category\",\n",
    "                \"age\": \"float32\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec093f-1fec-42fc-b1a4-f548b119cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _features_custs(df) -> list:\n",
    "    \"\"\"\n",
    "    generate customer features based on transaction sample\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of features\n",
    "    \"\"\"\n",
    "    n_sales_channel = df.sales_channel_id.nunique()\n",
    "    t_price = df.price.sum()\n",
    "    u_articles = df.article_id.nunique()\n",
    "    t_transactions = df.tx_month.count()\n",
    "    u_acive_days = df.t_dat.nunique()\n",
    "\n",
    "    return [n_sales_channel, t_price, u_articles, t_transactions, u_acive_days]\n",
    "\n",
    "\n",
    "def _features_articles(df) -> list:\n",
    "    \"\"\"\n",
    "    Generates article features based on transaction sample\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "    \"\"\"\n",
    "    last_purchase_date = df.t_dat.max()\n",
    "    first_purchase_date = df.t_dat.min()\n",
    "    u_customers = df.customer_id.nunique()\n",
    "    t_purchases = df.price.count()\n",
    "    latest_price = df[df.t_dat == last_purchase_date].price.max()\n",
    "    discount = df.price.max() - df.price.min()\n",
    "    article_availability = (last_purchase_date - first_purchase_date).days\n",
    "    median_age_buyers = df.age.median()\n",
    "\n",
    "    return [\n",
    "        u_customers,\n",
    "        t_purchases,\n",
    "        latest_price,\n",
    "        discount,\n",
    "        article_availability,\n",
    "        median_age_buyers,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2977e1c-e435-471a-8c78-b48669b6f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(input_, feature_max_date, months):\n",
    "    \"\"\"\n",
    "    Filter dataset based on the time period\n",
    "    \"\"\"\n",
    "    min_date = input_[feature_max_date].iloc[0] - pd.DateOffset(months=months)\n",
    "    max_date = input_[feature_max_date].iloc[0]\n",
    "\n",
    "    filtered_ = input_[input_[\"t_dat\"].between(min_date, max_date)]\n",
    "    return filtered_\n",
    "\n",
    "\n",
    "def compute_features(x, feature_max_date, agg_feature_name) -> list:\n",
    "    \"\"\"\n",
    "    Compute features for customer / product over following period\n",
    "    36: 3 years previous / lifetime\n",
    "    12: 12 months previous\n",
    "    6: 6 months previous\n",
    "    3: 3 months previous\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Pandas dataframe\n",
    "        groupby input dataframe for each customer/product\n",
    "    feature_max_date : String\n",
    "        date feature to select as a max date (for each dataset train, val and test)\n",
    "    agg_feature_name: String\n",
    "        Feature to use foor aggregation / groupby\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of computed features\n",
    "    \"\"\"\n",
    "\n",
    "    duration_list = [36, 12, 6, 3]\n",
    "\n",
    "    dfs_ = [get_df(x, feature_max_date, months) for months in duration_list]\n",
    "\n",
    "    if agg_feature_name == \"customer_id\":\n",
    "        features = [_features_custs(df_) for df_ in dfs_]\n",
    "    elif agg_feature_name == \"article_id\":\n",
    "        features = [_features_articles(df_) for df_ in dfs_]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f393781-984e-43b9-a7b5-c508b39ef08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timebased_features(row, path_model_data, sample=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the dataset from parquet file and applied feature computation\n",
    "    (this function is run in each process)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dummy_df : Pandas dataframe\n",
    "        Dummy dataset created to support parallel execution\n",
    "    path_model_data : String\n",
    "        path where model input parquet is stored\n",
    "    sample : bool, optional\n",
    "        A flag to generate a small sample for testing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    input_df = get_combined_data(path_model_data, sample=sample)\n",
    "    # remove null values for customer or articles not in the given dataset\n",
    "    input_df = input_df[~input_df[row.date_feature].isna()]\n",
    "\n",
    "    df_aggregates = input_df.groupby([row.agg_feature]).parallel_apply(\n",
    "        lambda x: compute_features(x, row.date_feature, row.agg_feature)\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    return df_aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e635fe-6714-4825-98a0-baaf69926a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features_df(x, path_model_data, col_features_cust, col_features_articles):\n",
    "    \"\"\"\n",
    "    Transforms the dataset obtained by parallel execution of feature generation\n",
    "    Process dataset per row\n",
    "    creates a new df for group by customer_id & article_id and 'train,val & test'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Pandas Series\n",
    "    col_features_cust : list\n",
    "        column names for customer df\n",
    "    col_features_articles : list\n",
    "        column name for article df\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    ARGS\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data_type = (\n",
    "        {\"customer_id\": \"int64\"}\n",
    "        if x.agg_feature.iloc[0] == \"customer_id\"\n",
    "        else {\"article_id\": \"int64\"}\n",
    "    )\n",
    "\n",
    "    ## since the group by is applied in parallel we can extract the only row\n",
    "    results_ = x.features.iloc[0].to_frame().rename(columns={0: \"features\"})\n",
    "    results_flatten = results_.features.apply(lambda x: np.ravel(x))\n",
    "    results_flatten = pd.DataFrame(\n",
    "        results_flatten.to_list(),\n",
    "        columns=col_features_cust\n",
    "        if x.agg_feature.iloc[0] == \"customer_id\"\n",
    "        else col_features_articles,\n",
    "        index=results_.index,\n",
    "    ).reset_index()\n",
    "\n",
    "    filename_ = (\n",
    "        x.dataset_type.iloc[0]\n",
    "        + \"_\"\n",
    "        + x.agg_feature.iloc[0].split(\"_\")[0]\n",
    "        + \"_tx_features.parquet\"\n",
    "    )\n",
    "\n",
    "    (\n",
    "        results_flatten.astype(data_type).to_parquet(\n",
    "            path_model_data / filename_, index=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96690b15-d382-4d07-958d-398e819a40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_computation(dummy_df, path_model_data, sample=False):\n",
    "    \"\"\"\n",
    "    Run feature generation pipeline\n",
    "    1. Fix feature column name for both customer and article\n",
    "    2. parallel execution of time partitioned feature generation\n",
    "    3. parallel execution of feature dataset transformation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dummy_df : Pandas DataFrame\n",
    "        Dummy dataset created to support parallel execution\n",
    "    path_model_data: String\n",
    "        path where model input parquet is stored\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # set colum names\n",
    "    col_features_cust = [\n",
    "        \"n_sales_channel\",\n",
    "        \"t_amt_spend\",\n",
    "        \"u_articles\",\n",
    "        \"t_transactions\",\n",
    "        \"u_acive_days\",\n",
    "    ]\n",
    "    col_features_articles = [\n",
    "        \"u_customers\",\n",
    "        \"t_purchases\",\n",
    "        \"latest_price\",\n",
    "        \"discount\",\n",
    "        \"article_availability\",\n",
    "        \"median_age_buyers\",\n",
    "    ]\n",
    "    col_names = [\"lt\", \"12m\", \"6m\", \"3m\"]\n",
    "\n",
    "    col_features_cust = [\n",
    "        col + \"_\" + value for value in col_names for col in col_features_cust\n",
    "    ]\n",
    "    col_features_articles = [\n",
    "        col + \"_\" + value for value in col_names for col in col_features_articles\n",
    "    ]\n",
    "\n",
    "    ## run parallel execution of feature generation\n",
    "    results_ = []\n",
    "    for row in dummy_df.itertuples():\n",
    "        print(row.agg_feature + \"------------\" + row.date_feature)\n",
    "        results_.append(get_timebased_features(row, path_model_data, sample=sample))\n",
    "\n",
    "    ## rehape input and write to file\n",
    "    temp_results_ = dummy_df.drop(columns=[\"index\"])\n",
    "    temp_results_[\"dataset_type\"] = temp_results_.date_feature.apply(\n",
    "        lambda x: str(x).split(\"_\")[0]\n",
    "    )\n",
    "    temp_results_[\"features\"] = results_\n",
    "\n",
    "    _ = temp_results_.groupby([\"agg_feature\", \"date_feature\"]).parallel_apply(\n",
    "        lambda x: transform_features_df(\n",
    "            x, path_model_data, col_features_cust, col_features_articles\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db25286-2243-410e-8775-c9f3f689dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "_ = run_feature_computation(dummy_df, path_model_data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab885a6-5824-4873-a19c-16ea4ca4de3a",
   "metadata": {},
   "source": [
    "### Test feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b627e2f-4cab-43a0-82df-63b619f8e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_(row, sample_data_):\n",
    "    \"\"\"\n",
    "    Generates dataset based on the time period and computes original features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : Pandas row (iterator)\n",
    "\n",
    "    sample_data_: Pandas dataframe\n",
    "        Dataframe subset for single customer or product\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    Features: dict\n",
    "        key value pair of item id (customer or article) and generated features\n",
    "    \"\"\"\n",
    "\n",
    "    def _foo(row, months, input_):\n",
    "        min_date = input_[row.date_feature].min() - pd.DateOffset(months=months)\n",
    "        max_date = input_[row.date_feature].min()\n",
    "\n",
    "        filtered_ = input_[input_[\"t_dat\"].between(min_date, max_date)]\n",
    "        return filtered_\n",
    "\n",
    "    duration_list = [36, 12, 6, 3]\n",
    "    dfs_ = [_foo(row, months, sample_data_) for months in duration_list]\n",
    "\n",
    "    if row.agg_feature == \"customer_id\":\n",
    "        features = [_features_custs(df_) for df_ in dfs_]\n",
    "        features = {sample_data_.customer_id.min(): features}\n",
    "\n",
    "    elif row.agg_feature == \"article_id\":\n",
    "        features = [_features_articles(df_) for df_ in dfs_]\n",
    "        features = {sample_data_.article_id.min(): features}\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6516ef-b0f0-4a4e-933d-626bbcd11a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_example(row, path_model_data, sample_data_):\n",
    "    \"\"\"\n",
    "    1. Reads combined dataframe\n",
    "    2. Take a random sample\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ## column names here matches the order of feature generation\n",
    "    ## col names also matches the order in duration\n",
    "    ## there fore the col_features_cust or col_features_articles will generate all features for single duration\n",
    "\n",
    "    col_names = [\"lt\", \"12m\", \"6m\", \"3m\"]\n",
    "    col_features_cust = [\n",
    "        \"n_sales_channel\",\n",
    "        \"t_amt_spend\",\n",
    "        \"u_articles\",\n",
    "        \"t_transactions\",\n",
    "        \"u_acive_days\",\n",
    "    ]\n",
    "    col_features_articles = [\n",
    "        \"u_customers\",\n",
    "        \"t_purchases\",\n",
    "        \"latest_price\",\n",
    "        \"discount\",\n",
    "        \"article_availability\",\n",
    "        \"median_age_buyers\",\n",
    "    ]\n",
    "    col_features_cust = [\n",
    "        col + \"_\" + value for value in col_names for col in col_features_cust\n",
    "    ]\n",
    "    col_features_articles = [\n",
    "        col + \"_\" + value for value in col_names for col in col_features_articles\n",
    "    ]\n",
    "\n",
    "    result = test_(row, sample_data_)\n",
    "    df_ = [\n",
    "        pd.DataFrame(\n",
    "            np.ravel(value).reshape(1, -1),\n",
    "            columns=col_features_cust\n",
    "            if row.agg_feature == \"customer_id\"\n",
    "            else col_features_articles,\n",
    "            index=[key],\n",
    "        )\n",
    "        for key, value in result.items()\n",
    "    ][0]\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26573758-5b32-41ba-9c26-4d07a80e8530",
   "metadata": {},
   "source": [
    "#### Run comparion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8033920a-33c3-4bf0-8b0f-64d4ccfc229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparision(row, dummy_df, path_model_data, test_data):\n",
    "    \"\"\" \"\"\"\n",
    "    filename_ = (\n",
    "        str(row.date_feature).split(\"_\")[0]\n",
    "        + \"_\"\n",
    "        + row.agg_feature.split(\"_\")[0]\n",
    "        + \"_tx_features.parquet\"\n",
    "    )\n",
    "    filepath = path_model_data / filename_\n",
    "    data_type = (\n",
    "        {\"customer_id\": \"int64\"}\n",
    "        if row.agg_feature == \"customer_id\"\n",
    "        else {\"article_id\": \"int64\"}\n",
    "    )\n",
    "\n",
    "    print(row.agg_feature + \"  ------  \" + row.date_feature)\n",
    "    computed_df = pd.read_parquet(filepath).astype(data_type)\n",
    "\n",
    "    if row.agg_feature == \"customer_id\":\n",
    "        test_item = computed_df.customer_id.sample(1).values[0]\n",
    "        sample_data_ = test_data[test_data.customer_id.isin([test_item])].copy()\n",
    "        org_df = computed_df[computed_df.customer_id == test_item].set_index(\n",
    "            [\"customer_id\"]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        test_item = computed_df.article_id.sample(1).values[0]\n",
    "        sample_data_ = test_data[test_data.article_id.isin([test_item])].copy()\n",
    "        org_df = computed_df[computed_df.article_id == test_item].set_index(\n",
    "            [\"article_id\"]\n",
    "        )\n",
    "\n",
    "    test_df = test_example(row, path_model_data, sample_data_)\n",
    "\n",
    "    try:\n",
    "        assert test_df.compare(org_df).sum().sum() == 0\n",
    "    except AssertionError:\n",
    "        return test_df, org_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd8f86-f3f7-419e-8222-c2b7095d00f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "test_data = get_combined_data(path_model_data)\n",
    "\n",
    "for i, row in enumerate(dummy_df.itertuples()):\n",
    "    run_comparision(row, dummy_df, path_model_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
